# Urdu-Text-Generation-using-GRU
Gated Recurrent Units (GRUs) were employed as a more computationally efficient alternative to LSTM. A stacked GRU architecture with two GRU layers and dropout layers was used. Additionally, pretrained FastText embeddings were integrated and fine-tuned to enhance performance.
